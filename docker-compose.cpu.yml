services:
  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llamacpp
    ports:
      - "11434:8080"
    volumes:
      - ./models:/models
      - ./config:/config
    environment:
      # CPU-only configuration optimized for i9-14900K (32 threads)
      - LLAMA_CPP_N_THREADS=24
      - LLAMA_CPP_N_THREADS_BATCH=24
      # Memory and context optimizations - use system RAM
      - LLAMA_CPP_N_CTX=16384
      - LLAMA_CPP_N_BATCH=512
      # No GPU layers for CPU mode
      - LLAMA_CPP_N_GPU_LAYERS=0
      # Server configuration
      - LLAMA_CPP_HOST=0.0.0.0
      - LLAMA_CPP_PORT=8080
      - LLAMA_CPP_PARALLEL=2
    deploy:
      resources:
        limits:
          memory: 64G
          cpus: '24'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s